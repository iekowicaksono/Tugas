{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#from sklearn.model_selection import train_test_split\n",
        "\n",
        "#dataset_path = r\"C:\\Users\\Dell\\Documents\\PhD\\Semester 3\\Pebelejaran Mesin Lanjut\\Tugas\\boston.csv\"\n",
        "#boston = pd.read_csv(dataset_path)\n",
        "\n",
        "#boston.head()\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data(\n",
        "    path=\"boston_housing.npz\", test_split=0.2, seed=113\n",
        ")\n",
        "\n",
        "x_train_df = pd.DataFrame(x_train, columns=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'])\n",
        "x_test_df = pd.DataFrame(x_test, columns=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'])\n",
        "y_train_df = pd.DataFrame(y_train, columns=['MEDV'])\n",
        "y_test_df = pd.DataFrame(y_test, columns=['MEDV'])\n",
        "#y_train_df.head()\n",
        "#y_train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train_df['MEDV'] = y_train_df\n",
        "x_test_df['MEDV'] = y_test_df\n",
        "#x_train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train_df['MEDV'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 64)                960       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               33280     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                32832     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 512)               33280     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109,185\n",
            "Trainable params: 109,185\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#x = boston[['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']]\n",
        "#y = boston['MEDV']\n",
        "\n",
        "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu', input_shape=(14,)))#,\n",
        "                               #kernel_regularizer=tf.keras.regularizers.L2()))\n",
        "#model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu')),\n",
        "model.add(tf.keras.layers.Dense(512, activation='tanh')),\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu')),\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu')),\n",
        "model.add(tf.keras.layers.Dense(512, activation='sigmoid')),\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(loss='mean_squared_logarithmic_error',\n",
        "             optimizer='Adam',\n",
        "             metrics=['mean_squared_logarithmic_error'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "6/6 [==============================] - 2s 68ms/step - loss: 5.1707 - mean_squared_logarithmic_error: 5.1707 - val_loss: 2.5591 - val_mean_squared_logarithmic_error: 2.5591\n",
            "Epoch 2/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 1.6337 - mean_squared_logarithmic_error: 1.6337 - val_loss: 0.8514 - val_mean_squared_logarithmic_error: 0.8514\n",
            "Epoch 3/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.5612 - mean_squared_logarithmic_error: 0.5612 - val_loss: 0.3636 - val_mean_squared_logarithmic_error: 0.3636\n",
            "Epoch 4/200\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.2666 - mean_squared_logarithmic_error: 0.2666 - val_loss: 0.2269 - val_mean_squared_logarithmic_error: 0.2269\n",
            "Epoch 5/200\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1937 - mean_squared_logarithmic_error: 0.1937 - val_loss: 0.1802 - val_mean_squared_logarithmic_error: 0.1802\n",
            "Epoch 6/200\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1701 - mean_squared_logarithmic_error: 0.1701 - val_loss: 0.1619 - val_mean_squared_logarithmic_error: 0.1619\n",
            "Epoch 7/200\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1606 - mean_squared_logarithmic_error: 0.1606 - val_loss: 0.1526 - val_mean_squared_logarithmic_error: 0.1526\n",
            "Epoch 8/200\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1566 - mean_squared_logarithmic_error: 0.1566 - val_loss: 0.1478 - val_mean_squared_logarithmic_error: 0.1478\n",
            "Epoch 9/200\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1543 - mean_squared_logarithmic_error: 0.1543 - val_loss: 0.1450 - val_mean_squared_logarithmic_error: 0.1450\n",
            "Epoch 10/200\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1531 - mean_squared_logarithmic_error: 0.1531 - val_loss: 0.1429 - val_mean_squared_logarithmic_error: 0.1429\n",
            "Epoch 11/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1522 - mean_squared_logarithmic_error: 0.1522 - val_loss: 0.1411 - val_mean_squared_logarithmic_error: 0.1411\n",
            "Epoch 12/200\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1515 - mean_squared_logarithmic_error: 0.1515 - val_loss: 0.1393 - val_mean_squared_logarithmic_error: 0.1393\n",
            "Epoch 13/200\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1509 - mean_squared_logarithmic_error: 0.1509 - val_loss: 0.1378 - val_mean_squared_logarithmic_error: 0.1378\n",
            "Epoch 14/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1503 - mean_squared_logarithmic_error: 0.1503 - val_loss: 0.1369 - val_mean_squared_logarithmic_error: 0.1369\n",
            "Epoch 15/200\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1500 - mean_squared_logarithmic_error: 0.1500 - val_loss: 0.1365 - val_mean_squared_logarithmic_error: 0.1365\n",
            "Epoch 16/200\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1498 - mean_squared_logarithmic_error: 0.1498 - val_loss: 0.1359 - val_mean_squared_logarithmic_error: 0.1359\n",
            "Epoch 17/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1497 - mean_squared_logarithmic_error: 0.1497 - val_loss: 0.1356 - val_mean_squared_logarithmic_error: 0.1356\n",
            "Epoch 18/200\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1496 - mean_squared_logarithmic_error: 0.1496 - val_loss: 0.1352 - val_mean_squared_logarithmic_error: 0.1352\n",
            "Epoch 19/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1494 - mean_squared_logarithmic_error: 0.1494 - val_loss: 0.1349 - val_mean_squared_logarithmic_error: 0.1349\n",
            "Epoch 20/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1493 - mean_squared_logarithmic_error: 0.1493 - val_loss: 0.1347 - val_mean_squared_logarithmic_error: 0.1347\n",
            "Epoch 21/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1493 - mean_squared_logarithmic_error: 0.1493 - val_loss: 0.1345 - val_mean_squared_logarithmic_error: 0.1345\n",
            "Epoch 22/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1492 - mean_squared_logarithmic_error: 0.1492 - val_loss: 0.1344 - val_mean_squared_logarithmic_error: 0.1344\n",
            "Epoch 23/200\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.1491 - mean_squared_logarithmic_error: 0.1491 - val_loss: 0.1343 - val_mean_squared_logarithmic_error: 0.1343\n",
            "Epoch 24/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1490 - mean_squared_logarithmic_error: 0.1490 - val_loss: 0.1342 - val_mean_squared_logarithmic_error: 0.1342\n",
            "Epoch 25/200\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.1490 - mean_squared_logarithmic_error: 0.1490 - val_loss: 0.1342 - val_mean_squared_logarithmic_error: 0.1342\n",
            "Epoch 26/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1489 - mean_squared_logarithmic_error: 0.1489 - val_loss: 0.1340 - val_mean_squared_logarithmic_error: 0.1340\n",
            "Epoch 27/200\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.1488 - mean_squared_logarithmic_error: 0.1488 - val_loss: 0.1339 - val_mean_squared_logarithmic_error: 0.1339\n",
            "Epoch 28/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1486 - mean_squared_logarithmic_error: 0.1486 - val_loss: 0.1341 - val_mean_squared_logarithmic_error: 0.1341\n",
            "Epoch 29/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1486 - mean_squared_logarithmic_error: 0.1486 - val_loss: 0.1345 - val_mean_squared_logarithmic_error: 0.1345\n",
            "Epoch 30/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1485 - mean_squared_logarithmic_error: 0.1485 - val_loss: 0.1344 - val_mean_squared_logarithmic_error: 0.1344\n",
            "Epoch 31/200\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.1483 - mean_squared_logarithmic_error: 0.1483 - val_loss: 0.1346 - val_mean_squared_logarithmic_error: 0.1346\n",
            "Epoch 32/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1483 - mean_squared_logarithmic_error: 0.1483 - val_loss: 0.1350 - val_mean_squared_logarithmic_error: 0.1350\n",
            "Epoch 33/200\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.1482 - mean_squared_logarithmic_error: 0.1482 - val_loss: 0.1350 - val_mean_squared_logarithmic_error: 0.1350\n",
            "Epoch 34/200\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.1480 - mean_squared_logarithmic_error: 0.1480 - val_loss: 0.1351 - val_mean_squared_logarithmic_error: 0.1351\n",
            "Epoch 35/200\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1477 - mean_squared_logarithmic_error: 0.1477 - val_loss: 0.1346 - val_mean_squared_logarithmic_error: 0.1346\n",
            "Epoch 36/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1473 - mean_squared_logarithmic_error: 0.1473 - val_loss: 0.1343 - val_mean_squared_logarithmic_error: 0.1343\n",
            "Epoch 37/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1470 - mean_squared_logarithmic_error: 0.1470 - val_loss: 0.1339 - val_mean_squared_logarithmic_error: 0.1339\n",
            "Epoch 38/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1466 - mean_squared_logarithmic_error: 0.1466 - val_loss: 0.1336 - val_mean_squared_logarithmic_error: 0.1336\n",
            "Epoch 39/200\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.1462 - mean_squared_logarithmic_error: 0.1462 - val_loss: 0.1334 - val_mean_squared_logarithmic_error: 0.1334\n",
            "Epoch 40/200\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.1458 - mean_squared_logarithmic_error: 0.1458 - val_loss: 0.1332 - val_mean_squared_logarithmic_error: 0.1332\n",
            "Epoch 41/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1453 - mean_squared_logarithmic_error: 0.1453 - val_loss: 0.1327 - val_mean_squared_logarithmic_error: 0.1327\n",
            "Epoch 42/200\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.1447 - mean_squared_logarithmic_error: 0.1447 - val_loss: 0.1318 - val_mean_squared_logarithmic_error: 0.1318\n",
            "Epoch 43/200\n",
            "6/6 [==============================] - 0s 27ms/step - loss: 0.1440 - mean_squared_logarithmic_error: 0.1440 - val_loss: 0.1310 - val_mean_squared_logarithmic_error: 0.1310\n",
            "Epoch 44/200\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.1432 - mean_squared_logarithmic_error: 0.1432 - val_loss: 0.1309 - val_mean_squared_logarithmic_error: 0.1309\n",
            "Epoch 45/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1418 - mean_squared_logarithmic_error: 0.1418 - val_loss: 0.1304 - val_mean_squared_logarithmic_error: 0.1304\n",
            "Epoch 46/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1407 - mean_squared_logarithmic_error: 0.1407 - val_loss: 0.1298 - val_mean_squared_logarithmic_error: 0.1298\n",
            "Epoch 47/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1387 - mean_squared_logarithmic_error: 0.1387 - val_loss: 0.1286 - val_mean_squared_logarithmic_error: 0.1286\n",
            "Epoch 48/200\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1367 - mean_squared_logarithmic_error: 0.1367 - val_loss: 0.1272 - val_mean_squared_logarithmic_error: 0.1272\n",
            "Epoch 49/200\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.1339 - mean_squared_logarithmic_error: 0.1339 - val_loss: 0.1263 - val_mean_squared_logarithmic_error: 0.1263\n",
            "Epoch 50/200\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1296 - mean_squared_logarithmic_error: 0.1296 - val_loss: 0.1262 - val_mean_squared_logarithmic_error: 0.1262\n",
            "Epoch 51/200\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1237 - mean_squared_logarithmic_error: 0.1237 - val_loss: 0.1242 - val_mean_squared_logarithmic_error: 0.1242\n",
            "Epoch 52/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1190 - mean_squared_logarithmic_error: 0.1190 - val_loss: 0.1160 - val_mean_squared_logarithmic_error: 0.1160\n",
            "Epoch 53/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1165 - mean_squared_logarithmic_error: 0.1165 - val_loss: 0.1162 - val_mean_squared_logarithmic_error: 0.1162\n",
            "Epoch 54/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1135 - mean_squared_logarithmic_error: 0.1135 - val_loss: 0.1162 - val_mean_squared_logarithmic_error: 0.1162\n",
            "Epoch 55/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1157 - mean_squared_logarithmic_error: 0.1157 - val_loss: 0.1094 - val_mean_squared_logarithmic_error: 0.1094\n",
            "Epoch 56/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1115 - mean_squared_logarithmic_error: 0.1115 - val_loss: 0.1118 - val_mean_squared_logarithmic_error: 0.1118\n",
            "Epoch 57/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1089 - mean_squared_logarithmic_error: 0.1089 - val_loss: 0.1087 - val_mean_squared_logarithmic_error: 0.1087\n",
            "Epoch 58/200\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.1064 - mean_squared_logarithmic_error: 0.1064 - val_loss: 0.1051 - val_mean_squared_logarithmic_error: 0.1051\n",
            "Epoch 59/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1144 - mean_squared_logarithmic_error: 0.1144 - val_loss: 0.1082 - val_mean_squared_logarithmic_error: 0.1082\n",
            "Epoch 60/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1253 - mean_squared_logarithmic_error: 0.1253 - val_loss: 0.1051 - val_mean_squared_logarithmic_error: 0.1051\n",
            "Epoch 61/200\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1137 - mean_squared_logarithmic_error: 0.1137 - val_loss: 0.1211 - val_mean_squared_logarithmic_error: 0.1211\n",
            "Epoch 62/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1070 - mean_squared_logarithmic_error: 0.1070 - val_loss: 0.1024 - val_mean_squared_logarithmic_error: 0.1024\n",
            "Epoch 63/200\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.1042 - mean_squared_logarithmic_error: 0.1042 - val_loss: 0.1041 - val_mean_squared_logarithmic_error: 0.1041\n",
            "Epoch 64/200\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1033 - mean_squared_logarithmic_error: 0.1033 - val_loss: 0.1036 - val_mean_squared_logarithmic_error: 0.1036\n",
            "Epoch 65/200\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.1005 - mean_squared_logarithmic_error: 0.1005 - val_loss: 0.1011 - val_mean_squared_logarithmic_error: 0.1011\n",
            "Epoch 66/200\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.0999 - mean_squared_logarithmic_error: 0.0999 - val_loss: 0.0999 - val_mean_squared_logarithmic_error: 0.0999\n",
            "Epoch 67/200\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.1117 - mean_squared_logarithmic_error: 0.1117 - val_loss: 0.1030 - val_mean_squared_logarithmic_error: 0.1030\n",
            "Epoch 68/200\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.1161 - mean_squared_logarithmic_error: 0.1161 - val_loss: 0.1002 - val_mean_squared_logarithmic_error: 0.1002\n",
            "Epoch 69/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1038 - mean_squared_logarithmic_error: 0.1038 - val_loss: 0.0986 - val_mean_squared_logarithmic_error: 0.0986\n",
            "Epoch 70/200\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.0992 - mean_squared_logarithmic_error: 0.0992 - val_loss: 0.0937 - val_mean_squared_logarithmic_error: 0.0937\n",
            "Epoch 71/200\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.0982 - mean_squared_logarithmic_error: 0.0982 - val_loss: 0.0928 - val_mean_squared_logarithmic_error: 0.0928\n",
            "Epoch 72/200\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0935 - mean_squared_logarithmic_error: 0.0935 - val_loss: 0.1131 - val_mean_squared_logarithmic_error: 0.1131\n",
            "Epoch 73/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1089 - mean_squared_logarithmic_error: 0.1089 - val_loss: 0.1032 - val_mean_squared_logarithmic_error: 0.1032\n",
            "Epoch 74/200\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0956 - mean_squared_logarithmic_error: 0.0956 - val_loss: 0.0896 - val_mean_squared_logarithmic_error: 0.0896\n",
            "Epoch 75/200\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.0981 - mean_squared_logarithmic_error: 0.0981 - val_loss: 0.0995 - val_mean_squared_logarithmic_error: 0.0995\n",
            "Epoch 76/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0958 - mean_squared_logarithmic_error: 0.0958 - val_loss: 0.0960 - val_mean_squared_logarithmic_error: 0.0960\n",
            "Epoch 77/200\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 0.0921 - mean_squared_logarithmic_error: 0.0921 - val_loss: 0.0947 - val_mean_squared_logarithmic_error: 0.0947\n",
            "Epoch 78/200\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0901 - mean_squared_logarithmic_error: 0.0901 - val_loss: 0.0943 - val_mean_squared_logarithmic_error: 0.0943\n",
            "Epoch 79/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0903 - mean_squared_logarithmic_error: 0.0903 - val_loss: 0.0938 - val_mean_squared_logarithmic_error: 0.0938\n",
            "Epoch 80/200\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.0876 - mean_squared_logarithmic_error: 0.0876 - val_loss: 0.0879 - val_mean_squared_logarithmic_error: 0.0879\n",
            "Epoch 81/200\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0861 - mean_squared_logarithmic_error: 0.0861 - val_loss: 0.0961 - val_mean_squared_logarithmic_error: 0.0961\n",
            "Epoch 82/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0893 - mean_squared_logarithmic_error: 0.0893 - val_loss: 0.0922 - val_mean_squared_logarithmic_error: 0.0922\n",
            "Epoch 83/200\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0843 - mean_squared_logarithmic_error: 0.0843 - val_loss: 0.0867 - val_mean_squared_logarithmic_error: 0.0867\n",
            "Epoch 84/200\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0827 - mean_squared_logarithmic_error: 0.0827 - val_loss: 0.0865 - val_mean_squared_logarithmic_error: 0.0865\n",
            "Epoch 85/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0801 - mean_squared_logarithmic_error: 0.0801 - val_loss: 0.0839 - val_mean_squared_logarithmic_error: 0.0839\n",
            "Epoch 86/200\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0789 - mean_squared_logarithmic_error: 0.0789 - val_loss: 0.0809 - val_mean_squared_logarithmic_error: 0.0809\n",
            "Epoch 87/200\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0850 - mean_squared_logarithmic_error: 0.0850 - val_loss: 0.0826 - val_mean_squared_logarithmic_error: 0.0826\n",
            "Epoch 88/200\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0823 - mean_squared_logarithmic_error: 0.0823 - val_loss: 0.0947 - val_mean_squared_logarithmic_error: 0.0947\n",
            "Epoch 89/200\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0850 - mean_squared_logarithmic_error: 0.0850 - val_loss: 0.0797 - val_mean_squared_logarithmic_error: 0.0797\n",
            "Epoch 90/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0878 - mean_squared_logarithmic_error: 0.0878 - val_loss: 0.0798 - val_mean_squared_logarithmic_error: 0.0798\n",
            "Epoch 91/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0812 - mean_squared_logarithmic_error: 0.0812 - val_loss: 0.0969 - val_mean_squared_logarithmic_error: 0.0969\n",
            "Epoch 92/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0829 - mean_squared_logarithmic_error: 0.0829 - val_loss: 0.0789 - val_mean_squared_logarithmic_error: 0.0789\n",
            "Epoch 93/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0743 - mean_squared_logarithmic_error: 0.0743 - val_loss: 0.0813 - val_mean_squared_logarithmic_error: 0.0813\n",
            "Epoch 94/200\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0728 - mean_squared_logarithmic_error: 0.0728 - val_loss: 0.0775 - val_mean_squared_logarithmic_error: 0.0775\n",
            "Epoch 95/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0704 - mean_squared_logarithmic_error: 0.0704 - val_loss: 0.0757 - val_mean_squared_logarithmic_error: 0.0757\n",
            "Epoch 96/200\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0698 - mean_squared_logarithmic_error: 0.0698 - val_loss: 0.0738 - val_mean_squared_logarithmic_error: 0.0738\n",
            "Epoch 97/200\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0705 - mean_squared_logarithmic_error: 0.0705 - val_loss: 0.0763 - val_mean_squared_logarithmic_error: 0.0763\n",
            "Epoch 98/200\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0680 - mean_squared_logarithmic_error: 0.0680 - val_loss: 0.0710 - val_mean_squared_logarithmic_error: 0.0710\n",
            "Epoch 99/200\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.0657 - mean_squared_logarithmic_error: 0.0657 - val_loss: 0.0739 - val_mean_squared_logarithmic_error: 0.0739\n",
            "Epoch 100/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0691 - mean_squared_logarithmic_error: 0.0691 - val_loss: 0.0767 - val_mean_squared_logarithmic_error: 0.0767\n",
            "Epoch 101/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0668 - mean_squared_logarithmic_error: 0.0668 - val_loss: 0.0718 - val_mean_squared_logarithmic_error: 0.0718\n",
            "Epoch 102/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0697 - mean_squared_logarithmic_error: 0.0697 - val_loss: 0.0744 - val_mean_squared_logarithmic_error: 0.0744\n",
            "Epoch 103/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0714 - mean_squared_logarithmic_error: 0.0714 - val_loss: 0.0695 - val_mean_squared_logarithmic_error: 0.0695\n",
            "Epoch 104/200\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0698 - mean_squared_logarithmic_error: 0.0698 - val_loss: 0.0671 - val_mean_squared_logarithmic_error: 0.0671\n",
            "Epoch 105/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0673 - mean_squared_logarithmic_error: 0.0673 - val_loss: 0.0662 - val_mean_squared_logarithmic_error: 0.0662\n",
            "Epoch 106/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0639 - mean_squared_logarithmic_error: 0.0639 - val_loss: 0.0659 - val_mean_squared_logarithmic_error: 0.0659\n",
            "Epoch 107/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0617 - mean_squared_logarithmic_error: 0.0617 - val_loss: 0.0642 - val_mean_squared_logarithmic_error: 0.0642\n",
            "Epoch 108/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0613 - mean_squared_logarithmic_error: 0.0613 - val_loss: 0.0666 - val_mean_squared_logarithmic_error: 0.0666\n",
            "Epoch 109/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0605 - mean_squared_logarithmic_error: 0.0605 - val_loss: 0.0745 - val_mean_squared_logarithmic_error: 0.0745\n",
            "Epoch 110/200\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0962 - mean_squared_logarithmic_error: 0.0962 - val_loss: 0.0643 - val_mean_squared_logarithmic_error: 0.0643\n",
            "Epoch 111/200\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0822 - mean_squared_logarithmic_error: 0.0822 - val_loss: 0.0647 - val_mean_squared_logarithmic_error: 0.0647\n",
            "Epoch 112/200\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0660 - mean_squared_logarithmic_error: 0.0660 - val_loss: 0.0671 - val_mean_squared_logarithmic_error: 0.0671\n",
            "Epoch 113/200\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0629 - mean_squared_logarithmic_error: 0.0629 - val_loss: 0.0645 - val_mean_squared_logarithmic_error: 0.0645\n",
            "Epoch 114/200\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.0594 - mean_squared_logarithmic_error: 0.0594 - val_loss: 0.0638 - val_mean_squared_logarithmic_error: 0.0638\n",
            "Epoch 115/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0580 - mean_squared_logarithmic_error: 0.0580 - val_loss: 0.0632 - val_mean_squared_logarithmic_error: 0.0632\n",
            "Epoch 116/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0566 - mean_squared_logarithmic_error: 0.0566 - val_loss: 0.0632 - val_mean_squared_logarithmic_error: 0.0632\n",
            "Epoch 117/200\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0556 - mean_squared_logarithmic_error: 0.0556 - val_loss: 0.0622 - val_mean_squared_logarithmic_error: 0.0622\n",
            "Epoch 118/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0554 - mean_squared_logarithmic_error: 0.0554 - val_loss: 0.0614 - val_mean_squared_logarithmic_error: 0.0614\n",
            "Epoch 119/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0560 - mean_squared_logarithmic_error: 0.0560 - val_loss: 0.0598 - val_mean_squared_logarithmic_error: 0.0598\n",
            "Epoch 120/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0546 - mean_squared_logarithmic_error: 0.0546 - val_loss: 0.0606 - val_mean_squared_logarithmic_error: 0.0606\n",
            "Epoch 121/200\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0543 - mean_squared_logarithmic_error: 0.0543 - val_loss: 0.0589 - val_mean_squared_logarithmic_error: 0.0589\n",
            "Epoch 122/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0537 - mean_squared_logarithmic_error: 0.0537 - val_loss: 0.0596 - val_mean_squared_logarithmic_error: 0.0596\n",
            "Epoch 123/200\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0527 - mean_squared_logarithmic_error: 0.0527 - val_loss: 0.0578 - val_mean_squared_logarithmic_error: 0.0578\n",
            "Epoch 124/200\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0515 - mean_squared_logarithmic_error: 0.0515 - val_loss: 0.0618 - val_mean_squared_logarithmic_error: 0.0618\n",
            "Epoch 125/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0517 - mean_squared_logarithmic_error: 0.0517 - val_loss: 0.0595 - val_mean_squared_logarithmic_error: 0.0595\n",
            "Epoch 126/200\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0525 - mean_squared_logarithmic_error: 0.0525 - val_loss: 0.0598 - val_mean_squared_logarithmic_error: 0.0598\n",
            "Epoch 127/200\n",
            "6/6 [==============================] - 0s 26ms/step - loss: 0.0536 - mean_squared_logarithmic_error: 0.0536 - val_loss: 0.0545 - val_mean_squared_logarithmic_error: 0.0545\n",
            "Epoch 128/200\n",
            "6/6 [==============================] - 0s 28ms/step - loss: 0.0529 - mean_squared_logarithmic_error: 0.0529 - val_loss: 0.0538 - val_mean_squared_logarithmic_error: 0.0538\n",
            "Epoch 129/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0513 - mean_squared_logarithmic_error: 0.0513 - val_loss: 0.0530 - val_mean_squared_logarithmic_error: 0.0530\n",
            "Epoch 130/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0483 - mean_squared_logarithmic_error: 0.0483 - val_loss: 0.0552 - val_mean_squared_logarithmic_error: 0.0552\n",
            "Epoch 131/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0493 - mean_squared_logarithmic_error: 0.0493 - val_loss: 0.0542 - val_mean_squared_logarithmic_error: 0.0542\n",
            "Epoch 132/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0522 - mean_squared_logarithmic_error: 0.0522 - val_loss: 0.0509 - val_mean_squared_logarithmic_error: 0.0509\n",
            "Epoch 133/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0568 - mean_squared_logarithmic_error: 0.0568 - val_loss: 0.0596 - val_mean_squared_logarithmic_error: 0.0596\n",
            "Epoch 134/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0512 - mean_squared_logarithmic_error: 0.0512 - val_loss: 0.0556 - val_mean_squared_logarithmic_error: 0.0556\n",
            "Epoch 135/200\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0495 - mean_squared_logarithmic_error: 0.0495 - val_loss: 0.0538 - val_mean_squared_logarithmic_error: 0.0538\n",
            "Epoch 136/200\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0475 - mean_squared_logarithmic_error: 0.0475 - val_loss: 0.0491 - val_mean_squared_logarithmic_error: 0.0491\n",
            "Epoch 137/200\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0466 - mean_squared_logarithmic_error: 0.0466 - val_loss: 0.0460 - val_mean_squared_logarithmic_error: 0.0460\n",
            "Epoch 138/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0433 - mean_squared_logarithmic_error: 0.0433 - val_loss: 0.0445 - val_mean_squared_logarithmic_error: 0.0445\n",
            "Epoch 139/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0412 - mean_squared_logarithmic_error: 0.0412 - val_loss: 0.0436 - val_mean_squared_logarithmic_error: 0.0436\n",
            "Epoch 140/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0404 - mean_squared_logarithmic_error: 0.0404 - val_loss: 0.0417 - val_mean_squared_logarithmic_error: 0.0417\n",
            "Epoch 141/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0399 - mean_squared_logarithmic_error: 0.0399 - val_loss: 0.0398 - val_mean_squared_logarithmic_error: 0.0398\n",
            "Epoch 142/200\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0382 - mean_squared_logarithmic_error: 0.0382 - val_loss: 0.0429 - val_mean_squared_logarithmic_error: 0.0429\n",
            "Epoch 143/200\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.0405 - mean_squared_logarithmic_error: 0.0405 - val_loss: 0.0412 - val_mean_squared_logarithmic_error: 0.0412\n",
            "Epoch 144/200\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0402 - mean_squared_logarithmic_error: 0.0402 - val_loss: 0.0379 - val_mean_squared_logarithmic_error: 0.0379\n",
            "Epoch 145/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0396 - mean_squared_logarithmic_error: 0.0396 - val_loss: 0.0399 - val_mean_squared_logarithmic_error: 0.0399\n",
            "Epoch 146/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0476 - mean_squared_logarithmic_error: 0.0476 - val_loss: 0.0392 - val_mean_squared_logarithmic_error: 0.0392\n",
            "Epoch 147/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0474 - mean_squared_logarithmic_error: 0.0474 - val_loss: 0.0638 - val_mean_squared_logarithmic_error: 0.0638\n",
            "Epoch 148/200\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0525 - mean_squared_logarithmic_error: 0.0525 - val_loss: 0.0477 - val_mean_squared_logarithmic_error: 0.0477\n",
            "Epoch 149/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0462 - mean_squared_logarithmic_error: 0.0462 - val_loss: 0.0521 - val_mean_squared_logarithmic_error: 0.0521\n",
            "Epoch 150/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0433 - mean_squared_logarithmic_error: 0.0433 - val_loss: 0.0527 - val_mean_squared_logarithmic_error: 0.0527\n",
            "Epoch 151/200\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0423 - mean_squared_logarithmic_error: 0.0423 - val_loss: 0.0451 - val_mean_squared_logarithmic_error: 0.0451\n",
            "Epoch 152/200\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0390 - mean_squared_logarithmic_error: 0.0390 - val_loss: 0.0378 - val_mean_squared_logarithmic_error: 0.0378\n",
            "Epoch 153/200\n",
            "6/6 [==============================] - 0s 28ms/step - loss: 0.0373 - mean_squared_logarithmic_error: 0.0373 - val_loss: 0.0403 - val_mean_squared_logarithmic_error: 0.0403\n",
            "Epoch 154/200\n",
            "6/6 [==============================] - 0s 30ms/step - loss: 0.0404 - mean_squared_logarithmic_error: 0.0404 - val_loss: 0.0450 - val_mean_squared_logarithmic_error: 0.0450\n",
            "Epoch 155/200\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0387 - mean_squared_logarithmic_error: 0.0387 - val_loss: 0.0442 - val_mean_squared_logarithmic_error: 0.0442\n",
            "Epoch 156/200\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0394 - mean_squared_logarithmic_error: 0.0394 - val_loss: 0.0416 - val_mean_squared_logarithmic_error: 0.0416\n",
            "Epoch 157/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0352 - mean_squared_logarithmic_error: 0.0352 - val_loss: 0.0376 - val_mean_squared_logarithmic_error: 0.0376\n",
            "Epoch 158/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0347 - mean_squared_logarithmic_error: 0.0347 - val_loss: 0.0369 - val_mean_squared_logarithmic_error: 0.0369\n",
            "Epoch 159/200\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0320 - mean_squared_logarithmic_error: 0.0320 - val_loss: 0.0335 - val_mean_squared_logarithmic_error: 0.0335\n",
            "Epoch 160/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0313 - mean_squared_logarithmic_error: 0.0313 - val_loss: 0.0347 - val_mean_squared_logarithmic_error: 0.0347\n",
            "Epoch 161/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0327 - mean_squared_logarithmic_error: 0.0327 - val_loss: 0.0449 - val_mean_squared_logarithmic_error: 0.0449\n",
            "Epoch 162/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0396 - mean_squared_logarithmic_error: 0.0396 - val_loss: 0.0388 - val_mean_squared_logarithmic_error: 0.0388\n",
            "Epoch 163/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0362 - mean_squared_logarithmic_error: 0.0362 - val_loss: 0.0380 - val_mean_squared_logarithmic_error: 0.0380\n",
            "Epoch 164/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0336 - mean_squared_logarithmic_error: 0.0336 - val_loss: 0.0328 - val_mean_squared_logarithmic_error: 0.0328\n",
            "Epoch 165/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0315 - mean_squared_logarithmic_error: 0.0315 - val_loss: 0.0332 - val_mean_squared_logarithmic_error: 0.0332\n",
            "Epoch 166/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0308 - mean_squared_logarithmic_error: 0.0308 - val_loss: 0.0359 - val_mean_squared_logarithmic_error: 0.0359\n",
            "Epoch 167/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0297 - mean_squared_logarithmic_error: 0.0297 - val_loss: 0.0320 - val_mean_squared_logarithmic_error: 0.0320\n",
            "Epoch 168/200\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0282 - mean_squared_logarithmic_error: 0.0282 - val_loss: 0.0299 - val_mean_squared_logarithmic_error: 0.0299\n",
            "Epoch 169/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0302 - mean_squared_logarithmic_error: 0.0302 - val_loss: 0.0311 - val_mean_squared_logarithmic_error: 0.0311\n",
            "Epoch 170/200\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.0283 - mean_squared_logarithmic_error: 0.0283 - val_loss: 0.0312 - val_mean_squared_logarithmic_error: 0.0312\n",
            "Epoch 171/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0281 - mean_squared_logarithmic_error: 0.0281 - val_loss: 0.0302 - val_mean_squared_logarithmic_error: 0.0302\n",
            "Epoch 172/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0261 - mean_squared_logarithmic_error: 0.0261 - val_loss: 0.0354 - val_mean_squared_logarithmic_error: 0.0354\n",
            "Epoch 173/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0300 - mean_squared_logarithmic_error: 0.0300 - val_loss: 0.0307 - val_mean_squared_logarithmic_error: 0.0307\n",
            "Epoch 174/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0298 - mean_squared_logarithmic_error: 0.0298 - val_loss: 0.0316 - val_mean_squared_logarithmic_error: 0.0316\n",
            "Epoch 175/200\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0316 - mean_squared_logarithmic_error: 0.0316 - val_loss: 0.0293 - val_mean_squared_logarithmic_error: 0.0293\n",
            "Epoch 176/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0285 - mean_squared_logarithmic_error: 0.0285 - val_loss: 0.0315 - val_mean_squared_logarithmic_error: 0.0315\n",
            "Epoch 177/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0290 - mean_squared_logarithmic_error: 0.0290 - val_loss: 0.0271 - val_mean_squared_logarithmic_error: 0.0271\n",
            "Epoch 178/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0259 - mean_squared_logarithmic_error: 0.0259 - val_loss: 0.0301 - val_mean_squared_logarithmic_error: 0.0301\n",
            "Epoch 179/200\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.0257 - mean_squared_logarithmic_error: 0.0257 - val_loss: 0.0282 - val_mean_squared_logarithmic_error: 0.0282\n",
            "Epoch 180/200\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0247 - mean_squared_logarithmic_error: 0.0247 - val_loss: 0.0294 - val_mean_squared_logarithmic_error: 0.0294\n",
            "Epoch 181/200\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0255 - mean_squared_logarithmic_error: 0.0255 - val_loss: 0.0282 - val_mean_squared_logarithmic_error: 0.0282\n",
            "Epoch 182/200\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0268 - mean_squared_logarithmic_error: 0.0268 - val_loss: 0.0270 - val_mean_squared_logarithmic_error: 0.0270\n",
            "Epoch 183/200\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.0284 - mean_squared_logarithmic_error: 0.0284 - val_loss: 0.0390 - val_mean_squared_logarithmic_error: 0.0390\n",
            "Epoch 184/200\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0328 - mean_squared_logarithmic_error: 0.0328 - val_loss: 0.0304 - val_mean_squared_logarithmic_error: 0.0304\n",
            "Epoch 185/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0270 - mean_squared_logarithmic_error: 0.0270 - val_loss: 0.0281 - val_mean_squared_logarithmic_error: 0.0281\n",
            "Epoch 186/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0276 - mean_squared_logarithmic_error: 0.0276 - val_loss: 0.0327 - val_mean_squared_logarithmic_error: 0.0327\n",
            "Epoch 187/200\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0268 - mean_squared_logarithmic_error: 0.0268 - val_loss: 0.0305 - val_mean_squared_logarithmic_error: 0.0305\n",
            "Epoch 188/200\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.0283 - mean_squared_logarithmic_error: 0.0283 - val_loss: 0.0325 - val_mean_squared_logarithmic_error: 0.0325\n",
            "Epoch 189/200\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0268 - mean_squared_logarithmic_error: 0.0268 - val_loss: 0.0271 - val_mean_squared_logarithmic_error: 0.0271\n",
            "Epoch 190/200\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0241 - mean_squared_logarithmic_error: 0.0241 - val_loss: 0.0265 - val_mean_squared_logarithmic_error: 0.0265\n",
            "Epoch 191/200\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0221 - mean_squared_logarithmic_error: 0.0221 - val_loss: 0.0254 - val_mean_squared_logarithmic_error: 0.0254\n",
            "Epoch 192/200\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.0221 - mean_squared_logarithmic_error: 0.0221 - val_loss: 0.0246 - val_mean_squared_logarithmic_error: 0.0246\n",
            "Epoch 193/200\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0214 - mean_squared_logarithmic_error: 0.0214 - val_loss: 0.0256 - val_mean_squared_logarithmic_error: 0.0256\n",
            "Epoch 194/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0210 - mean_squared_logarithmic_error: 0.0210 - val_loss: 0.0244 - val_mean_squared_logarithmic_error: 0.0244\n",
            "Epoch 195/200\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0210 - mean_squared_logarithmic_error: 0.0210 - val_loss: 0.0246 - val_mean_squared_logarithmic_error: 0.0246\n",
            "Epoch 196/200\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0214 - mean_squared_logarithmic_error: 0.0214 - val_loss: 0.0279 - val_mean_squared_logarithmic_error: 0.0279\n",
            "Epoch 197/200\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0221 - mean_squared_logarithmic_error: 0.0221 - val_loss: 0.0231 - val_mean_squared_logarithmic_error: 0.0231\n",
            "Epoch 198/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0207 - mean_squared_logarithmic_error: 0.0207 - val_loss: 0.0246 - val_mean_squared_logarithmic_error: 0.0246\n",
            "Epoch 199/200\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.0220 - mean_squared_logarithmic_error: 0.0220 - val_loss: 0.0258 - val_mean_squared_logarithmic_error: 0.0258\n",
            "Epoch 200/200\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0215 - mean_squared_logarithmic_error: 0.0215 - val_loss: 0.0255 - val_mean_squared_logarithmic_error: 0.0255\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'loss': [5.170691967010498,\n",
              "  1.6337414979934692,\n",
              "  0.5611922144889832,\n",
              "  0.2666124701499939,\n",
              "  0.1937342882156372,\n",
              "  0.1700773686170578,\n",
              "  0.16063979268074036,\n",
              "  0.1566159874200821,\n",
              "  0.15434785187244415,\n",
              "  0.15309680998325348,\n",
              "  0.15220937132835388,\n",
              "  0.15151819586753845,\n",
              "  0.15085969865322113,\n",
              "  0.15031087398529053,\n",
              "  0.15004657208919525,\n",
              "  0.14984187483787537,\n",
              "  0.1497219204902649,\n",
              "  0.1495703160762787,\n",
              "  0.14940379559993744,\n",
              "  0.14930155873298645,\n",
              "  0.14925512671470642,\n",
              "  0.14918452501296997,\n",
              "  0.14908260107040405,\n",
              "  0.1490088850259781,\n",
              "  0.1489708423614502,\n",
              "  0.14892397820949554,\n",
              "  0.14875462651252747,\n",
              "  0.14863668382167816,\n",
              "  0.1485803872346878,\n",
              "  0.14845913648605347,\n",
              "  0.1483146697282791,\n",
              "  0.14830264449119568,\n",
              "  0.14817875623703003,\n",
              "  0.14804455637931824,\n",
              "  0.147726908326149,\n",
              "  0.14732442796230316,\n",
              "  0.14698489010334015,\n",
              "  0.1465870887041092,\n",
              "  0.14621564745903015,\n",
              "  0.14583231508731842,\n",
              "  0.1452726274728775,\n",
              "  0.14469003677368164,\n",
              "  0.14402905106544495,\n",
              "  0.14315804839134216,\n",
              "  0.141805499792099,\n",
              "  0.14072895050048828,\n",
              "  0.13873402774333954,\n",
              "  0.13673825562000275,\n",
              "  0.1339266151189804,\n",
              "  0.12960724532604218,\n",
              "  0.12366864830255508,\n",
              "  0.11900609731674194,\n",
              "  0.11653874069452286,\n",
              "  0.11347876489162445,\n",
              "  0.1156843826174736,\n",
              "  0.11146943271160126,\n",
              "  0.10886684060096741,\n",
              "  0.10642936825752258,\n",
              "  0.11444227397441864,\n",
              "  0.1253451406955719,\n",
              "  0.11369878798723221,\n",
              "  0.10701590776443481,\n",
              "  0.10417161136865616,\n",
              "  0.10334166139364243,\n",
              "  0.10052695870399475,\n",
              "  0.0998501256108284,\n",
              "  0.11168242990970612,\n",
              "  0.11612081527709961,\n",
              "  0.10382405668497086,\n",
              "  0.0992160513997078,\n",
              "  0.09820611774921417,\n",
              "  0.09351398050785065,\n",
              "  0.10892975330352783,\n",
              "  0.09561806917190552,\n",
              "  0.09813976287841797,\n",
              "  0.09578943252563477,\n",
              "  0.09214333444833755,\n",
              "  0.09013552963733673,\n",
              "  0.09030124545097351,\n",
              "  0.0876164361834526,\n",
              "  0.08612152189016342,\n",
              "  0.08932408690452576,\n",
              "  0.08428807556629181,\n",
              "  0.08266977965831757,\n",
              "  0.08007631450891495,\n",
              "  0.07888895273208618,\n",
              "  0.08503815531730652,\n",
              "  0.08225800842046738,\n",
              "  0.084996297955513,\n",
              "  0.08775726705789566,\n",
              "  0.08116345852613449,\n",
              "  0.08287833631038666,\n",
              "  0.0742800161242485,\n",
              "  0.07281829416751862,\n",
              "  0.07039105892181396,\n",
              "  0.06977599859237671,\n",
              "  0.07053916156291962,\n",
              "  0.06798247247934341,\n",
              "  0.06567589938640594,\n",
              "  0.06908854097127914,\n",
              "  0.06680641323328018,\n",
              "  0.06967899948358536,\n",
              "  0.07135547697544098,\n",
              "  0.06983678042888641,\n",
              "  0.06734638661146164,\n",
              "  0.06391042470932007,\n",
              "  0.061682987958192825,\n",
              "  0.06134897097945213,\n",
              "  0.060515984892845154,\n",
              "  0.09618871659040451,\n",
              "  0.08221038430929184,\n",
              "  0.06602179259061813,\n",
              "  0.0628562644124031,\n",
              "  0.05944362282752991,\n",
              "  0.05801881104707718,\n",
              "  0.056570760905742645,\n",
              "  0.05564587935805321,\n",
              "  0.05538240075111389,\n",
              "  0.05598984286189079,\n",
              "  0.05464623495936394,\n",
              "  0.0542914979159832,\n",
              "  0.053667228668928146,\n",
              "  0.052685581147670746,\n",
              "  0.051532573997974396,\n",
              "  0.05167653411626816,\n",
              "  0.052506592124700546,\n",
              "  0.053553756326436996,\n",
              "  0.05293431505560875,\n",
              "  0.05126219242811203,\n",
              "  0.04831200838088989,\n",
              "  0.049298301339149475,\n",
              "  0.05220580846071243,\n",
              "  0.0568373017013073,\n",
              "  0.051185160875320435,\n",
              "  0.049540936946868896,\n",
              "  0.04752757400274277,\n",
              "  0.0465971902012825,\n",
              "  0.0433000773191452,\n",
              "  0.04118398204445839,\n",
              "  0.040365345776081085,\n",
              "  0.039850443601608276,\n",
              "  0.038216833025217056,\n",
              "  0.04048587381839752,\n",
              "  0.04023425653576851,\n",
              "  0.03959240764379501,\n",
              "  0.04758055508136749,\n",
              "  0.047445815056562424,\n",
              "  0.052514322102069855,\n",
              "  0.04622186720371246,\n",
              "  0.04331387206912041,\n",
              "  0.042268067598342896,\n",
              "  0.039002854377031326,\n",
              "  0.03725411370396614,\n",
              "  0.04038301110267639,\n",
              "  0.038731060922145844,\n",
              "  0.0394069105386734,\n",
              "  0.03524316847324371,\n",
              "  0.034727759659290314,\n",
              "  0.03202763572335243,\n",
              "  0.03134692832827568,\n",
              "  0.03271937370300293,\n",
              "  0.03957284986972809,\n",
              "  0.03623512014746666,\n",
              "  0.03356127440929413,\n",
              "  0.03146993741393089,\n",
              "  0.03079376369714737,\n",
              "  0.029664674773812294,\n",
              "  0.028239678591489792,\n",
              "  0.03017856366932392,\n",
              "  0.028285888954997063,\n",
              "  0.02806069515645504,\n",
              "  0.026064317673444748,\n",
              "  0.029993057250976562,\n",
              "  0.029765350744128227,\n",
              "  0.03156203031539917,\n",
              "  0.028475049883127213,\n",
              "  0.02895365282893181,\n",
              "  0.02594408392906189,\n",
              "  0.02568827196955681,\n",
              "  0.024708041921257973,\n",
              "  0.02546653524041176,\n",
              "  0.026809873059391975,\n",
              "  0.028355836868286133,\n",
              "  0.03284062072634697,\n",
              "  0.027003206312656403,\n",
              "  0.02755136974155903,\n",
              "  0.02675078809261322,\n",
              "  0.028302600607275963,\n",
              "  0.026815157383680344,\n",
              "  0.024066893383860588,\n",
              "  0.02211921662092209,\n",
              "  0.0221042949706316,\n",
              "  0.021387150511145592,\n",
              "  0.02101503685116768,\n",
              "  0.020967558026313782,\n",
              "  0.0213969387114048,\n",
              "  0.022146249189972878,\n",
              "  0.02074464224278927,\n",
              "  0.021976709365844727,\n",
              "  0.021511657163500786],\n",
              " 'mean_squared_logarithmic_error': [5.170691967010498,\n",
              "  1.6337414979934692,\n",
              "  0.5611922144889832,\n",
              "  0.2666124701499939,\n",
              "  0.1937342882156372,\n",
              "  0.1700773686170578,\n",
              "  0.16063979268074036,\n",
              "  0.1566159874200821,\n",
              "  0.15434785187244415,\n",
              "  0.15309680998325348,\n",
              "  0.15220937132835388,\n",
              "  0.15151819586753845,\n",
              "  0.15085969865322113,\n",
              "  0.15031087398529053,\n",
              "  0.15004657208919525,\n",
              "  0.14984187483787537,\n",
              "  0.1497219204902649,\n",
              "  0.1495703160762787,\n",
              "  0.14940379559993744,\n",
              "  0.14930155873298645,\n",
              "  0.14925512671470642,\n",
              "  0.14918452501296997,\n",
              "  0.14908260107040405,\n",
              "  0.1490088850259781,\n",
              "  0.1489708423614502,\n",
              "  0.14892397820949554,\n",
              "  0.14875462651252747,\n",
              "  0.14863668382167816,\n",
              "  0.1485803872346878,\n",
              "  0.14845913648605347,\n",
              "  0.1483146697282791,\n",
              "  0.14830264449119568,\n",
              "  0.14817875623703003,\n",
              "  0.14804455637931824,\n",
              "  0.147726908326149,\n",
              "  0.14732442796230316,\n",
              "  0.14698489010334015,\n",
              "  0.1465870887041092,\n",
              "  0.14621564745903015,\n",
              "  0.14583231508731842,\n",
              "  0.1452726274728775,\n",
              "  0.14469003677368164,\n",
              "  0.14402905106544495,\n",
              "  0.14315804839134216,\n",
              "  0.141805499792099,\n",
              "  0.14072895050048828,\n",
              "  0.13873402774333954,\n",
              "  0.13673825562000275,\n",
              "  0.1339266151189804,\n",
              "  0.12960724532604218,\n",
              "  0.12366864830255508,\n",
              "  0.11900609731674194,\n",
              "  0.11653874069452286,\n",
              "  0.11347876489162445,\n",
              "  0.1156843826174736,\n",
              "  0.11146943271160126,\n",
              "  0.10886684060096741,\n",
              "  0.10642936825752258,\n",
              "  0.11444227397441864,\n",
              "  0.1253451406955719,\n",
              "  0.11369878798723221,\n",
              "  0.10701590776443481,\n",
              "  0.10417161136865616,\n",
              "  0.10334166139364243,\n",
              "  0.10052695870399475,\n",
              "  0.0998501256108284,\n",
              "  0.11168242990970612,\n",
              "  0.11612081527709961,\n",
              "  0.10382405668497086,\n",
              "  0.0992160513997078,\n",
              "  0.09820611774921417,\n",
              "  0.09351398050785065,\n",
              "  0.10892975330352783,\n",
              "  0.09561806917190552,\n",
              "  0.09813976287841797,\n",
              "  0.09578943252563477,\n",
              "  0.09214333444833755,\n",
              "  0.09013552963733673,\n",
              "  0.09030124545097351,\n",
              "  0.0876164361834526,\n",
              "  0.08612152189016342,\n",
              "  0.08932408690452576,\n",
              "  0.08428807556629181,\n",
              "  0.08266977965831757,\n",
              "  0.08007631450891495,\n",
              "  0.07888895273208618,\n",
              "  0.08503815531730652,\n",
              "  0.08225800842046738,\n",
              "  0.084996297955513,\n",
              "  0.08775726705789566,\n",
              "  0.08116345852613449,\n",
              "  0.08287833631038666,\n",
              "  0.0742800161242485,\n",
              "  0.07281829416751862,\n",
              "  0.07039105892181396,\n",
              "  0.06977599859237671,\n",
              "  0.07053916156291962,\n",
              "  0.06798247247934341,\n",
              "  0.06567589938640594,\n",
              "  0.06908854097127914,\n",
              "  0.06680641323328018,\n",
              "  0.06967899948358536,\n",
              "  0.07135547697544098,\n",
              "  0.06983678042888641,\n",
              "  0.06734638661146164,\n",
              "  0.06391042470932007,\n",
              "  0.061682987958192825,\n",
              "  0.06134897097945213,\n",
              "  0.060515984892845154,\n",
              "  0.09618871659040451,\n",
              "  0.08221038430929184,\n",
              "  0.06602179259061813,\n",
              "  0.0628562644124031,\n",
              "  0.05944362282752991,\n",
              "  0.05801881104707718,\n",
              "  0.056570760905742645,\n",
              "  0.05564587935805321,\n",
              "  0.05538240075111389,\n",
              "  0.05598984286189079,\n",
              "  0.05464623495936394,\n",
              "  0.0542914979159832,\n",
              "  0.053667228668928146,\n",
              "  0.052685581147670746,\n",
              "  0.051532573997974396,\n",
              "  0.05167653411626816,\n",
              "  0.052506592124700546,\n",
              "  0.053553756326436996,\n",
              "  0.05293431505560875,\n",
              "  0.05126219242811203,\n",
              "  0.04831200838088989,\n",
              "  0.049298301339149475,\n",
              "  0.05220580846071243,\n",
              "  0.0568373017013073,\n",
              "  0.051185160875320435,\n",
              "  0.049540936946868896,\n",
              "  0.04752757400274277,\n",
              "  0.0465971902012825,\n",
              "  0.0433000773191452,\n",
              "  0.04118398204445839,\n",
              "  0.040365345776081085,\n",
              "  0.039850443601608276,\n",
              "  0.038216833025217056,\n",
              "  0.04048587381839752,\n",
              "  0.04023425653576851,\n",
              "  0.03959240764379501,\n",
              "  0.04758055508136749,\n",
              "  0.047445815056562424,\n",
              "  0.052514322102069855,\n",
              "  0.04622186720371246,\n",
              "  0.04331387206912041,\n",
              "  0.042268067598342896,\n",
              "  0.039002854377031326,\n",
              "  0.03725411370396614,\n",
              "  0.04038301110267639,\n",
              "  0.038731060922145844,\n",
              "  0.0394069105386734,\n",
              "  0.03524316847324371,\n",
              "  0.034727759659290314,\n",
              "  0.03202763572335243,\n",
              "  0.03134692832827568,\n",
              "  0.03271937370300293,\n",
              "  0.03957284986972809,\n",
              "  0.03623512014746666,\n",
              "  0.03356127440929413,\n",
              "  0.03146993741393089,\n",
              "  0.03079376369714737,\n",
              "  0.029664674773812294,\n",
              "  0.028239678591489792,\n",
              "  0.03017856366932392,\n",
              "  0.028285888954997063,\n",
              "  0.02806069515645504,\n",
              "  0.026064317673444748,\n",
              "  0.029993057250976562,\n",
              "  0.029765350744128227,\n",
              "  0.03156203031539917,\n",
              "  0.028475049883127213,\n",
              "  0.02895365282893181,\n",
              "  0.02594408392906189,\n",
              "  0.02568827196955681,\n",
              "  0.024708041921257973,\n",
              "  0.02546653524041176,\n",
              "  0.026809873059391975,\n",
              "  0.028355836868286133,\n",
              "  0.03284062072634697,\n",
              "  0.027003206312656403,\n",
              "  0.02755136974155903,\n",
              "  0.02675078809261322,\n",
              "  0.028302600607275963,\n",
              "  0.026815157383680344,\n",
              "  0.024066893383860588,\n",
              "  0.02211921662092209,\n",
              "  0.0221042949706316,\n",
              "  0.021387150511145592,\n",
              "  0.02101503685116768,\n",
              "  0.020967558026313782,\n",
              "  0.0213969387114048,\n",
              "  0.022146249189972878,\n",
              "  0.02074464224278927,\n",
              "  0.021976709365844727,\n",
              "  0.021511657163500786],\n",
              " 'val_loss': [2.5590968132019043,\n",
              "  0.8513508439064026,\n",
              "  0.36355406045913696,\n",
              "  0.22685131430625916,\n",
              "  0.18015390634536743,\n",
              "  0.1618984043598175,\n",
              "  0.15256744623184204,\n",
              "  0.14780400693416595,\n",
              "  0.14500844478607178,\n",
              "  0.1428719013929367,\n",
              "  0.14113380014896393,\n",
              "  0.13933387398719788,\n",
              "  0.13779345154762268,\n",
              "  0.13691018521785736,\n",
              "  0.1364576667547226,\n",
              "  0.13591404259204865,\n",
              "  0.13557478785514832,\n",
              "  0.13523907959461212,\n",
              "  0.1348777413368225,\n",
              "  0.13468778133392334,\n",
              "  0.13454751670360565,\n",
              "  0.13440898060798645,\n",
              "  0.13427187502384186,\n",
              "  0.13418762385845184,\n",
              "  0.13416822254657745,\n",
              "  0.13395962119102478,\n",
              "  0.13394375145435333,\n",
              "  0.13413970172405243,\n",
              "  0.13447265326976776,\n",
              "  0.13435281813144684,\n",
              "  0.13455034792423248,\n",
              "  0.13498979806900024,\n",
              "  0.13501085340976715,\n",
              "  0.13510000705718994,\n",
              "  0.13459786772727966,\n",
              "  0.13434070348739624,\n",
              "  0.13389119505882263,\n",
              "  0.13355521857738495,\n",
              "  0.13342241942882538,\n",
              "  0.1331920623779297,\n",
              "  0.13270746171474457,\n",
              "  0.13183802366256714,\n",
              "  0.13096296787261963,\n",
              "  0.13089591264724731,\n",
              "  0.13035722076892853,\n",
              "  0.1297517865896225,\n",
              "  0.12862448394298553,\n",
              "  0.12719447910785675,\n",
              "  0.1262892335653305,\n",
              "  0.12623631954193115,\n",
              "  0.12423410266637802,\n",
              "  0.11604384332895279,\n",
              "  0.1161825880408287,\n",
              "  0.11615250259637833,\n",
              "  0.10937648266553879,\n",
              "  0.11182921379804611,\n",
              "  0.10868585854768753,\n",
              "  0.10511094331741333,\n",
              "  0.10816182941198349,\n",
              "  0.10505682229995728,\n",
              "  0.12106858938932419,\n",
              "  0.10237361490726471,\n",
              "  0.10409292578697205,\n",
              "  0.10358475893735886,\n",
              "  0.1010645180940628,\n",
              "  0.0999414324760437,\n",
              "  0.10302363336086273,\n",
              "  0.1002403274178505,\n",
              "  0.0986381471157074,\n",
              "  0.09367536008358002,\n",
              "  0.09279218316078186,\n",
              "  0.11312217265367508,\n",
              "  0.10316953808069229,\n",
              "  0.08959943056106567,\n",
              "  0.09952731430530548,\n",
              "  0.09595464915037155,\n",
              "  0.09466840326786041,\n",
              "  0.09434999525547028,\n",
              "  0.09376085549592972,\n",
              "  0.08790875226259232,\n",
              "  0.09607826918363571,\n",
              "  0.09223583340644836,\n",
              "  0.08666025847196579,\n",
              "  0.08645442873239517,\n",
              "  0.0838637724518776,\n",
              "  0.08086054772138596,\n",
              "  0.08260507881641388,\n",
              "  0.09465492516756058,\n",
              "  0.07971532642841339,\n",
              "  0.07975178956985474,\n",
              "  0.09688878059387207,\n",
              "  0.07886487245559692,\n",
              "  0.0812893882393837,\n",
              "  0.07753849774599075,\n",
              "  0.07567404210567474,\n",
              "  0.07379109412431717,\n",
              "  0.07627326250076294,\n",
              "  0.07097643613815308,\n",
              "  0.07385288178920746,\n",
              "  0.07668882608413696,\n",
              "  0.07176624983549118,\n",
              "  0.07436931133270264,\n",
              "  0.06948374956846237,\n",
              "  0.06709662824869156,\n",
              "  0.06624697148799896,\n",
              "  0.06589273363351822,\n",
              "  0.06417272239923477,\n",
              "  0.06655741482973099,\n",
              "  0.07452895492315292,\n",
              "  0.06433463096618652,\n",
              "  0.06466586142778397,\n",
              "  0.06712298840284348,\n",
              "  0.06445597112178802,\n",
              "  0.06380131840705872,\n",
              "  0.06322340667247772,\n",
              "  0.06321083754301071,\n",
              "  0.062221281230449677,\n",
              "  0.061378803104162216,\n",
              "  0.059815794229507446,\n",
              "  0.06056845560669899,\n",
              "  0.058913372457027435,\n",
              "  0.05958161503076553,\n",
              "  0.05778248608112335,\n",
              "  0.061777740716934204,\n",
              "  0.059454966336488724,\n",
              "  0.05984098091721535,\n",
              "  0.05453837290406227,\n",
              "  0.053760141134262085,\n",
              "  0.05296830087900162,\n",
              "  0.05519469454884529,\n",
              "  0.054184556007385254,\n",
              "  0.05089523643255234,\n",
              "  0.059558846056461334,\n",
              "  0.055588457733392715,\n",
              "  0.05378492549061775,\n",
              "  0.04910874739289284,\n",
              "  0.0459849014878273,\n",
              "  0.04453873634338379,\n",
              "  0.043641261756420135,\n",
              "  0.041665252298116684,\n",
              "  0.039820972830057144,\n",
              "  0.042858004570007324,\n",
              "  0.04121684283018112,\n",
              "  0.03793034330010414,\n",
              "  0.03985842317342758,\n",
              "  0.03920309245586395,\n",
              "  0.0638015940785408,\n",
              "  0.0476742684841156,\n",
              "  0.05208374559879303,\n",
              "  0.05271953344345093,\n",
              "  0.04508526250720024,\n",
              "  0.03777462989091873,\n",
              "  0.040256988257169724,\n",
              "  0.04499567300081253,\n",
              "  0.044177230447530746,\n",
              "  0.04157665744423866,\n",
              "  0.03757411614060402,\n",
              "  0.03689725324511528,\n",
              "  0.03353440761566162,\n",
              "  0.034651994705200195,\n",
              "  0.044922418892383575,\n",
              "  0.038800228387117386,\n",
              "  0.03803997486829758,\n",
              "  0.032774705439805984,\n",
              "  0.03316590189933777,\n",
              "  0.03588945046067238,\n",
              "  0.03196455538272858,\n",
              "  0.029932690784335136,\n",
              "  0.031101079657673836,\n",
              "  0.031223125755786896,\n",
              "  0.030217261984944344,\n",
              "  0.035382237285375595,\n",
              "  0.03066614829003811,\n",
              "  0.0315740741789341,\n",
              "  0.02934041991829872,\n",
              "  0.0314522460103035,\n",
              "  0.027124864980578423,\n",
              "  0.03008265607059002,\n",
              "  0.028210202232003212,\n",
              "  0.02943316288292408,\n",
              "  0.028248442336916924,\n",
              "  0.026988470926880836,\n",
              "  0.03903009742498398,\n",
              "  0.030377551913261414,\n",
              "  0.02806931361556053,\n",
              "  0.03268531337380409,\n",
              "  0.030542120337486267,\n",
              "  0.03247814252972603,\n",
              "  0.027102405205368996,\n",
              "  0.026489490643143654,\n",
              "  0.025371242314577103,\n",
              "  0.024614959955215454,\n",
              "  0.025579750537872314,\n",
              "  0.024390418082475662,\n",
              "  0.02459016442298889,\n",
              "  0.027877017855644226,\n",
              "  0.023093195632100105,\n",
              "  0.024613242596387863,\n",
              "  0.025820255279541016,\n",
              "  0.025488438084721565],\n",
              " 'val_mean_squared_logarithmic_error': [2.5590968132019043,\n",
              "  0.8513508439064026,\n",
              "  0.36355406045913696,\n",
              "  0.22685131430625916,\n",
              "  0.18015390634536743,\n",
              "  0.1618984043598175,\n",
              "  0.15256744623184204,\n",
              "  0.14780400693416595,\n",
              "  0.14500844478607178,\n",
              "  0.1428719013929367,\n",
              "  0.14113380014896393,\n",
              "  0.13933387398719788,\n",
              "  0.13779345154762268,\n",
              "  0.13691018521785736,\n",
              "  0.1364576667547226,\n",
              "  0.13591404259204865,\n",
              "  0.13557478785514832,\n",
              "  0.13523907959461212,\n",
              "  0.1348777413368225,\n",
              "  0.13468778133392334,\n",
              "  0.13454751670360565,\n",
              "  0.13440898060798645,\n",
              "  0.13427187502384186,\n",
              "  0.13418762385845184,\n",
              "  0.13416822254657745,\n",
              "  0.13395962119102478,\n",
              "  0.13394375145435333,\n",
              "  0.13413970172405243,\n",
              "  0.13447265326976776,\n",
              "  0.13435281813144684,\n",
              "  0.13455034792423248,\n",
              "  0.13498979806900024,\n",
              "  0.13501085340976715,\n",
              "  0.13510000705718994,\n",
              "  0.13459786772727966,\n",
              "  0.13434070348739624,\n",
              "  0.13389119505882263,\n",
              "  0.13355521857738495,\n",
              "  0.13342241942882538,\n",
              "  0.1331920623779297,\n",
              "  0.13270746171474457,\n",
              "  0.13183802366256714,\n",
              "  0.13096296787261963,\n",
              "  0.13089591264724731,\n",
              "  0.13035722076892853,\n",
              "  0.1297517865896225,\n",
              "  0.12862448394298553,\n",
              "  0.12719447910785675,\n",
              "  0.1262892335653305,\n",
              "  0.12623631954193115,\n",
              "  0.12423410266637802,\n",
              "  0.11604384332895279,\n",
              "  0.1161825880408287,\n",
              "  0.11615250259637833,\n",
              "  0.10937648266553879,\n",
              "  0.11182921379804611,\n",
              "  0.10868585854768753,\n",
              "  0.10511094331741333,\n",
              "  0.10816182941198349,\n",
              "  0.10505682229995728,\n",
              "  0.12106858938932419,\n",
              "  0.10237361490726471,\n",
              "  0.10409292578697205,\n",
              "  0.10358475893735886,\n",
              "  0.1010645180940628,\n",
              "  0.0999414324760437,\n",
              "  0.10302363336086273,\n",
              "  0.1002403199672699,\n",
              "  0.0986381471157074,\n",
              "  0.09367536008358002,\n",
              "  0.09279218316078186,\n",
              "  0.11312217265367508,\n",
              "  0.10316953808069229,\n",
              "  0.08959943056106567,\n",
              "  0.09952731430530548,\n",
              "  0.09595464915037155,\n",
              "  0.09466840326786041,\n",
              "  0.09434999525547028,\n",
              "  0.09376085549592972,\n",
              "  0.08790875226259232,\n",
              "  0.09607826918363571,\n",
              "  0.09223583340644836,\n",
              "  0.08666025847196579,\n",
              "  0.08645442873239517,\n",
              "  0.0838637724518776,\n",
              "  0.08086054772138596,\n",
              "  0.08260507881641388,\n",
              "  0.09465492516756058,\n",
              "  0.07971532642841339,\n",
              "  0.07975178956985474,\n",
              "  0.09688878059387207,\n",
              "  0.07886487245559692,\n",
              "  0.0812893882393837,\n",
              "  0.07753849774599075,\n",
              "  0.07567404210567474,\n",
              "  0.07379109412431717,\n",
              "  0.07627326250076294,\n",
              "  0.07097643613815308,\n",
              "  0.07385288178920746,\n",
              "  0.07668882608413696,\n",
              "  0.07176624983549118,\n",
              "  0.07436931133270264,\n",
              "  0.06948374956846237,\n",
              "  0.06709662824869156,\n",
              "  0.06624697148799896,\n",
              "  0.06589273363351822,\n",
              "  0.06417272239923477,\n",
              "  0.06655741482973099,\n",
              "  0.07452895492315292,\n",
              "  0.06433463096618652,\n",
              "  0.06466586142778397,\n",
              "  0.06712298840284348,\n",
              "  0.06445597112178802,\n",
              "  0.06380131840705872,\n",
              "  0.06322340667247772,\n",
              "  0.06321083754301071,\n",
              "  0.062221281230449677,\n",
              "  0.061378803104162216,\n",
              "  0.059815794229507446,\n",
              "  0.06056845560669899,\n",
              "  0.058913372457027435,\n",
              "  0.05958161503076553,\n",
              "  0.05778248608112335,\n",
              "  0.061777740716934204,\n",
              "  0.059454966336488724,\n",
              "  0.05984098091721535,\n",
              "  0.05453837290406227,\n",
              "  0.053760141134262085,\n",
              "  0.05296830087900162,\n",
              "  0.05519469454884529,\n",
              "  0.054184556007385254,\n",
              "  0.05089523643255234,\n",
              "  0.059558846056461334,\n",
              "  0.055588457733392715,\n",
              "  0.05378492549061775,\n",
              "  0.04910874739289284,\n",
              "  0.0459849014878273,\n",
              "  0.04453873634338379,\n",
              "  0.043641261756420135,\n",
              "  0.041665252298116684,\n",
              "  0.039820972830057144,\n",
              "  0.042858004570007324,\n",
              "  0.04121684283018112,\n",
              "  0.03793034330010414,\n",
              "  0.03985842317342758,\n",
              "  0.03920309245586395,\n",
              "  0.0638015940785408,\n",
              "  0.0476742684841156,\n",
              "  0.05208374559879303,\n",
              "  0.05271953344345093,\n",
              "  0.04508526250720024,\n",
              "  0.03777462989091873,\n",
              "  0.040256988257169724,\n",
              "  0.04499567300081253,\n",
              "  0.044177230447530746,\n",
              "  0.04157665744423866,\n",
              "  0.03757411614060402,\n",
              "  0.03689725324511528,\n",
              "  0.03353440761566162,\n",
              "  0.034651994705200195,\n",
              "  0.044922418892383575,\n",
              "  0.038800228387117386,\n",
              "  0.03803997486829758,\n",
              "  0.032774705439805984,\n",
              "  0.03316590189933777,\n",
              "  0.03588945046067238,\n",
              "  0.03196455538272858,\n",
              "  0.029932690784335136,\n",
              "  0.031101079657673836,\n",
              "  0.031223125755786896,\n",
              "  0.030217261984944344,\n",
              "  0.035382237285375595,\n",
              "  0.03066614829003811,\n",
              "  0.0315740741789341,\n",
              "  0.02934041991829872,\n",
              "  0.0314522460103035,\n",
              "  0.027124864980578423,\n",
              "  0.03008265607059002,\n",
              "  0.028210202232003212,\n",
              "  0.02943316288292408,\n",
              "  0.028248442336916924,\n",
              "  0.026988470926880836,\n",
              "  0.03903009742498398,\n",
              "  0.030377551913261414,\n",
              "  0.02806931361556053,\n",
              "  0.03268531337380409,\n",
              "  0.030542120337486267,\n",
              "  0.03247814252972603,\n",
              "  0.027102405205368996,\n",
              "  0.026489490643143654,\n",
              "  0.025371242314577103,\n",
              "  0.024614959955215454,\n",
              "  0.025579750537872314,\n",
              "  0.024390418082475662,\n",
              "  0.02459016442298889,\n",
              "  0.027877017855644226,\n",
              "  0.023093195632100105,\n",
              "  0.024613242596387863,\n",
              "  0.025820255279541016,\n",
              "  0.025488438084721565]}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "start_time = datetime.now()\n",
        "\n",
        "history = model.fit(x_train_df, y_train, epochs=200, batch_size=64, validation_split=0.2)\n",
        "end_time = datetime.now()\n",
        "\n",
        "history.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0248 - mean_squared_logarithmic_error: 0.0248\n",
            "Loss, Accuracy:  [0.0248086117208004, 0.0248086117208004]\n",
            "Training Duration: 0:00:23.468694\n"
          ]
        }
      ],
      "source": [
        "result = model.evaluate(x_test_df, y_test)\n",
        "print('Loss, Accuracy: ', result)\n",
        "print('Training Duration: {}'.format(end_time - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Xmf_JRJa_N8C"
      ],
      "name": "Part1_MNIST_Solution.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
