{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKA_J7bdP33T"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 MIT 6.S191 Introduction to Deep Learning. All Rights Reserved.\n",
        "# \n",
        "# Licensed under the MIT License. You may not use this file except in compliance\n",
        "# with the License. Use and/or modification of this code outside of 6.S191 must\n",
        "# reference:\n",
        "#\n",
        "# Â© MIT 6.S191: Introduction to Deep Learning\n",
        "# http://introtodeeplearning.com\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#! pip install opencv-contrib-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RsGqx_ai_N8F"
      },
      "outputs": [],
      "source": [
        "# Import Tensorflow 2.0\n",
        "#tensorflow_version 2.x\n",
        "import tensorflow as tf \n",
        "import cv2\n",
        "\n",
        "#! pip install mitdeeplearning\n",
        "import mitdeeplearning as mdl\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check that we are using a GPU, if not switch runtimes\n",
        "#   using Runtime > Change Runtime Type > GPU\n",
        "# assert len(tf.config.list_physical_devices('GPU')) > 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2dQsHI3_N8K"
      },
      "outputs": [],
      "source": [
        "#mnist = tf.keras.datasets.mnist\n",
        "#Training Fashion MNIST Dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "train_images = (np.expand_dims(train_images, axis=-1)/255.).astype(np.float32)\n",
        "train_labels = (train_labels).astype(np.int64)\n",
        "test_images = (np.expand_dims(test_images, axis=-1)/255.).astype(np.float32)\n",
        "test_labels = (test_labels).astype(np.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDBsR2lP_N8O",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "random_inds = np.random.choice(60000,36)\n",
        "for i in range(36):\n",
        "    plt.subplot(6,6,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    image_ind = random_inds[i]\n",
        "    plt.imshow(np.squeeze(train_images[image_ind]), cmap=plt.cm.binary)\n",
        "    plt.xlabel(train_labels[image_ind])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMZsbjAkDKpU"
      },
      "outputs": [],
      "source": [
        "def build_fc_model():\n",
        "  fc_model = tf.keras.Sequential([\n",
        "      # First define a Flatten layer\n",
        "      tf.keras.layers.Flatten(),\n",
        "\n",
        "      # '''TODO: Define the activation function for the first fully connected (Dense) layer.'''\n",
        "      tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "      # tf.keras.layers.Dense(128, activation= '''TODO'''),\n",
        "\n",
        "      # '''TODO: Define the second Dense layer to output the classification probabilities'''\n",
        "      tf.keras.layers.Dense(10, activation=tf.nn.softmax) \n",
        "      # [TODO Dense layer to output classification probabilities]\n",
        "      \n",
        "  ])\n",
        "  return fc_model\n",
        "\n",
        "model = build_fc_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lhan11blCaW7"
      },
      "outputs": [],
      "source": [
        "'''TODO: Experiment with different optimizers and learning rates. How do these affect\n",
        "    the accuracy of the trained model? Which optimizers and/or learning rates yield\n",
        "    the best performance?'''\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-1), \n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFMbIqIvQ2X0"
      },
      "outputs": [],
      "source": [
        "# Define the batch size and the number of epochs to use during training\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "\n",
        "model.fit(train_images, train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VflXLEeECaXC"
      },
      "outputs": [],
      "source": [
        "'''TODO: Use the evaluate method to test the model!'''\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels) # TODO\n",
        "# test_loss, test_acc = # TODO\n",
        "\n",
        "print('Test accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vec9qcJs-9W5"
      },
      "outputs": [],
      "source": [
        "def build_cnn_model():\n",
        "    cnn_model = tf.keras.Sequential([\n",
        "\n",
        "        # TODO: Define the first convolutional layer\n",
        "        tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation=tf.nn.relu),\n",
        "        tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation=tf.nn.relu),\n",
        "        #tf.keras.layers.Conv2D(filters=24, kernel_size=(3,3), activation=tf.nn.relu),\n",
        "        #tf.keras.layers.Conv2D(filters=24, kernel_size=(3,3), activation=tf.nn.relu),\n",
        "        #tf.keras.layers.Conv2D(filters=24, kernel_size=(3,3), activation=tf.nn.relu),\n",
        "        # tf.keras.layers.Conv2D('''TODO''') \n",
        "\n",
        "        # TODO: Define the first max pooling layer\n",
        "        #tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "        # tf.keras.layers.MaxPool2D('''TODO''')\n",
        "\n",
        "        # TODO: Define the second convolutional layer\n",
        "        #tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation=tf.nn.relu),\n",
        "        # tf.keras.layers.Conv2D('''TODO''')\n",
        "\n",
        "        # TODO: Define the second max pooling layer\n",
        "        tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "        # tf.keras.layers.MaxPool2D('''TODO''')\n",
        "\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "        #tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "        #tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "        #tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "\n",
        "        # TODO: Define the last Dense layer to output the classification \n",
        "        # probabilities. Pay attention to the activation needed a probability\n",
        "        # output\n",
        "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "        # [TODO Dense layer to output classification probabilities]\n",
        "    ])\n",
        "    \n",
        "    return cnn_model\n",
        "  \n",
        "cnn_model = build_cnn_model()\n",
        "# Initialize the model by passing some data through\n",
        "cnn_model.predict(train_images[[0]])\n",
        "# Print the summary of the layers in the model.\n",
        "print(cnn_model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vheyanDkCg6a"
      },
      "outputs": [],
      "source": [
        "'''TODO: Define the batch size and the number of epochs to use during training'''\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "\n",
        "'''TODO: Define learning rate scheduller'''\n",
        "initial_learning_rate = 1e-3\n",
        "decay = initial_learning_rate/EPOCHS\n",
        "\n",
        "'''TODO: Define the compile operation with your optimizer and learning rate of choice'''\n",
        "cnn_model.compile(optimizer=tf.keras.optimizers.Ftrl(learning_rate=1e-3),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "# cnn_model.compile(optimizer='''TODO''', loss='''TODO''', metrics=['accuracy']) # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdrGZVmWDK4p"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "start_time = datetime.now()\n",
        "'''TODO: Use model.fit to train the CNN model, with the same batch_size and number of epochs previously used.'''\n",
        "cnn_model.fit(train_images, train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS)\n",
        "# cnn_model.fit('''TODO''')\n",
        "end_time = datetime.now()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDm4znZcDtNl"
      },
      "outputs": [],
      "source": [
        "'''TODO: Use the evaluate method to test the model!'''\n",
        "test_loss, test_acc = cnn_model.evaluate(test_images, test_labels)\n",
        "# test_loss, test_acc = # TODO\n",
        "\n",
        "print('Test accuracy:', test_acc)\n",
        "print('Duration: {}'.format(end_time - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl91RPhdCaXI"
      },
      "outputs": [],
      "source": [
        "predictions = cnn_model.predict(test_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DmJEUinCaXK"
      },
      "outputs": [],
      "source": [
        "predictions[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsqenuPnCaXO"
      },
      "outputs": [],
      "source": [
        "'''TODO: identify the digit with the highest confidence prediction for the first\n",
        "    image in the test dataset. '''\n",
        "prediction = np.argmax(predictions[0]) \n",
        "# prediction = # TODO\n",
        "\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sd7Pgsu6CaXP"
      },
      "outputs": [],
      "source": [
        "print(\"Label of this digit is:\", test_labels[0])\n",
        "plt.imshow(test_images[0,:,:,0], cmap=plt.cm.binary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HV5jw-5HwSmO"
      },
      "outputs": [],
      "source": [
        "#@title Change the slider to look at the model's predictions! { run: \"auto\" }\n",
        "\n",
        "image_index = 79 #@param {type:\"slider\", min:0, max:100, step:1}\n",
        "plt.subplot(1,2,1)\n",
        "mdl.lab2.plot_image_prediction(image_index, predictions, test_labels, test_images)\n",
        "plt.subplot(1,2,2)\n",
        "mdl.lab2.plot_value_prediction(image_index, predictions,  test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQlnbqaw2Qu_"
      },
      "outputs": [],
      "source": [
        "# Plots the first X test images, their predicted label, and the true label\n",
        "# Color correct predictions in blue, incorrect predictions in red\n",
        "num_rows = 5\n",
        "num_cols = 4\n",
        "num_images = num_rows*num_cols\n",
        "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
        "for i in range(num_images):\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
        "  mdl.lab2.plot_image_prediction(i, predictions, test_labels, test_images)\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
        "  mdl.lab2.plot_value_prediction(i, predictions, test_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wq34id-iN1Ml"
      },
      "outputs": [],
      "source": [
        "# Rebuild the CNN model\n",
        "cnn_model = build_cnn_model()\n",
        "\n",
        "batch_size = 12\n",
        "loss_history = mdl.util.LossHistory(smoothing_factor=0.95) # to record the evolution of the loss\n",
        "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss', scale='semilogy')\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2) # define our optimizer\n",
        "\n",
        "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
        "\n",
        "for idx in tqdm(range(0, train_images.shape[0], batch_size)):\n",
        "  # First grab a batch of training data and convert the input images to tensors\n",
        "  (images, labels) = (train_images[idx:idx+batch_size], train_labels[idx:idx+batch_size])\n",
        "  images = tf.convert_to_tensor(images, dtype=tf.float32)\n",
        "\n",
        "  # GradientTape to record differentiation operations\n",
        "  with tf.GradientTape() as tape:\n",
        "    #'''TODO: feed the images into the model and obtain the predictions'''\n",
        "    logits = cnn_model(images)\n",
        "    # logits = # TODO\n",
        "\n",
        "    #'''TODO: compute the categorical cross entropy loss\n",
        "    loss_value = tf.keras.backend.sparse_categorical_crossentropy(labels, logits)\n",
        "    # loss_value = tf.keras.backend.sparse_categorical_crossentropy() # TODO\n",
        "\n",
        "  loss_history.append(loss_value.numpy().mean()) # append the loss to the loss_history record\n",
        "  plotter.plot(loss_history.get())\n",
        "\n",
        "  # Backpropagation\n",
        "  '''TODO: Use the tape to compute the gradient against all parameters in the CNN model.\n",
        "      Use cnn_model.trainable_variables to access these parameters.''' \n",
        "  grads = tape.gradient(loss_value, cnn_model.trainable_variables)\n",
        "  # grads = # TODO\n",
        "  optimizer.apply_gradients(zip(grads, cnn_model.trainable_variables))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Xmf_JRJa_N8C"
      ],
      "name": "Part1_MNIST_Solution.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
